# Architecture

This document describes the system design, module organization, and data flow for the Balkan NLP dataset pipeline.

---

## ğŸ¯ Design Principles

1. **Configuration-driven**: Business logic in code, parameters in YAML
2. **Separation of concerns**: Each module has one clear responsibility
3. **Reproducibility**: Same config â†’ same output
4. **Transparency**: Every step is logged and auditable
5. **Modularity**: Components can be used independently

---

## ğŸ“Š High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         PIPELINE                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Sources (Web/Files) â†’ Scraping â†’ Processing â†’ Export       â”‚
â”‚                           â†“           â†“           â†“          â”‚
â”‚                        Cache      Intermediate  Output       â”‚
â”‚                                      Files      (JSONL)      â”‚
â”‚                                                   â†“          â”‚
â”‚                                            Hugging Face      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—‚ï¸ Module Organization

### 1. scraping/

**Purpose**: Fetch and extract raw content from web sources.

**Responsibilities**:

- HTTP requests with rate limiting
- HTML parsing and boilerplate removal
- Metadata extraction (date, source, URL)
- Error handling and retries
- Caching responses

**Does NOT**:

- Clean text
- Deduplicate
- Transform data

**Key Files**:

```
scraping/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ fetch.py              # HTTP client with rate limiting
â”œâ”€â”€ extract.py            # Content extraction (trafilatura wrapper)
â””â”€â”€ sources/
    â”œâ”€â”€ common.py         # Shared utilities
    â”œâ”€â”€ klix.py           # Site-specific scraper
    â”œâ”€â”€ index.py          # Site-specific scraper
    â””â”€â”€ blic.py           # Site-specific scraper
```

**Interface**:

```python
def scrape_source(url: str, config: dict) -> dict:
    """Scrape single URL.

    Returns:
        {
            'text': str,
            'title': str,
            'date': datetime,
            'url': str,
            'metadata': dict
        }
    """
```

---

### 2. processing/

**Purpose**: Transform raw scraped content into clean, structured data.

**Responsibilities**:

- Text cleaning (HTML, whitespace, unicode)
- Normalization (quotes, dashes, case)
- Deduplication (exact + near-duplicate)
- Language identification validation
- Quality filtering
- Dataset splitting (train/val/test)

**Does NOT**:

- Fetch data from web
- Write to disk
- Upload to Hugging Face

**Key Files**:

```
processing/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ cleaning.py           # Text cleaning functions
â”œâ”€â”€ normalization.py      # Unicode, whitespace normalization
â”œâ”€â”€ deduplication.py      # MinHash, SHA256 dedup
â”œâ”€â”€ language_check.py     # FastText validation
â””â”€â”€ splitting.py          # Train/val/test splits
```

**Interface**:

```python
def clean_text(text: str, config: dict) -> str:
    """Clean and normalize text."""

def deduplicate(documents: list[dict], config: dict) -> list[dict]:
    """Remove duplicate and near-duplicate documents."""

def split_dataset(data: list[dict], config: dict) -> dict:
    """Split into train/val/test with stratification."""
```

---

### 3. export/

**Purpose**: Format and export processed data to various formats and destinations.

**Responsibilities**:

- Serialize to JSONL, Parquet
- Compress outputs
- Generate checksums
- Upload to Hugging Face
- Create dataset cards

**Does NOT**:

- Process or transform data
- Validate data quality

**Key Files**:

```
export/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ to_jsonl.py           # JSONL export
â”œâ”€â”€ to_parquet.py         # Parquet export
â””â”€â”€ hf_upload.py          # Hugging Face integration
```

**Interface**:

```python
def to_jsonl(data: list[dict], output_path: str, compress: bool = True):
    """Export to JSONL format."""

def upload_to_hf(data: list[dict], repo_name: str, config: dict):
    """Upload to Hugging Face with dataset card."""
```

---

### 4. utils/

**Purpose**: Shared utilities used across modules.

**Responsibilities**:

- Logging setup
- Text utilities (counting, tokenization)
- Hashing functions
- Configuration loading
- File I/O helpers

**Key Files**:

```
utils/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ logging.py            # Logging configuration
â”œâ”€â”€ text_utils.py         # Text manipulation
â”œâ”€â”€ hashing.py            # SHA256, MinHash
â””â”€â”€ config.py             # YAML config loading
```

---

### 5. scripts/

**Purpose**: Entry points for running pipelines.

**Responsibilities**:

- CLI argument parsing
- Loading configurations
- Orchestrating pipeline steps
- Progress reporting

**Key Files**:

```
scripts/
â”œâ”€â”€ run_clean_text.py     # Phase 1: Clean text corpus
â”œâ”€â”€ run_language_id.py    # Phase 1: Language ID dataset
â””â”€â”€ run_summarization.py  # Phase 1: Summarization dataset
```

**Example**:

```python
# scripts/run_clean_text.py
import argparse
from scraping import scrape_sources
from processing import clean_text, deduplicate
from export import to_jsonl, upload_to_hf

def main():
    args = parse_args()
    config = load_config(args.config)

    # Pipeline
    raw_data = scrape_sources(config['sources'])
    cleaned = clean_text(raw_data, config['cleaning'])
    deduped = deduplicate(cleaned, config['dedup'])

    # Export
    to_jsonl(deduped, config['output']['path'])
    upload_to_hf(deduped, config['output']['hf_repo'])

if __name__ == '__main__':
    main()
```

---

### 6. datasets/

**Purpose**: Configuration files for each dataset (NOT data storage).

**Structure**:

```
datasets/
â”œâ”€â”€ clean_text/
â”‚   â”œâ”€â”€ README.md         # Dataset documentation
â”‚   â”œâ”€â”€ config.yaml       # Processing configuration
â”‚   â””â”€â”€ sources.yaml      # Source URLs
â”‚
â”œâ”€â”€ language_id/
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ config.yaml
â”‚
â””â”€â”€ summarization/
    â”œâ”€â”€ README.md
    â””â”€â”€ config.yaml
```

**CRITICAL**: This directory contains ONLY configuration and documentation, NEVER actual data files.

---

## ğŸ”„ Data Flow

### Phase 1: Clean Text Corpus

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  News Sites  â”‚
â”‚  Wikipedia   â”‚
â”‚  Gov Portals â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  scraping/   â”‚  â† sources.yaml
â”‚  fetch.py    â”‚  â† Rate limiting, robots.txt
â”‚  extract.py  â”‚  â† Trafilatura extraction
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“ Raw HTML â†’ Clean text + metadata
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ processing/  â”‚
â”‚ cleaning.py  â”‚  â† Remove boilerplate, normalize
â”‚ dedup.py     â”‚  â† MinHash + SHA256
â”‚ lang_check   â”‚  â† FastText validation
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“ Deduplicated documents
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   export/    â”‚
â”‚ to_jsonl.py  â”‚  â† Format + compress
â”‚ hf_upload.py â”‚  â† Upload to HF
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Hugging Face â”‚
â”‚   Dataset    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Derived Datasets (Language ID, Summarization)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Clean Text      â”‚  â† Base dataset
â”‚  Corpus          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                 â”‚
         â†“                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Language ID   â”‚  â”‚ Summarizationâ”‚
â”‚  Extraction    â”‚  â”‚  Lead Extractâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                 â”‚
         â†“                 â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ HF LID â”‚        â”‚ HF Sum â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Configuration System

### Hierarchy

```
1. defaults (in code)
2. config.yaml (dataset-specific)
3. config.local.yaml (gitignored, overrides)
4. environment variables (for secrets)
```

### Example: datasets/clean_text/config.yaml

```yaml
dataset:
    name: sr-bs-hr-clean-text
    version: 1.0.0

collection:
    user_agent: "BalkanNLP/1.0"
    timeout: 30
    max_retries: 3
    rate_limit: 1

cleaning:
    min_length: 200
    max_length: 50000
    unicode_normalization: NFC

deduplication:
    use_sha256: true
    use_minhash: true
    minhash_threshold: 0.90

output:
    formats: [jsonl, parquet]
    compression: gzip
    hf_repo: "balkan-nlp/sr-bs-hr-clean-text"
```

---

## ğŸ“¦ Dependencies

### Core

- `requests` - HTTP client
- `trafilatura` - Web content extraction
- `beautifulsoup4` - HTML parsing fallback
- `pyyaml` - Configuration files

### Processing

- `datasets` - Hugging Face integration
- `pandas` - Data manipulation
- `datasketch` - MinHash deduplication
- `fasttext` - Language identification

### Export

- `pyarrow` - Parquet format
- `huggingface-hub` - HF uploads

---

## ğŸš¦ Error Handling Strategy

### Levels

1. **Retry** - Network errors, timeouts (with backoff)
2. **Skip** - Invalid URLs, 404s (log and continue)
3. **Fail** - Configuration errors, missing dependencies (stop pipeline)

### Example

```python
# Retry with backoff
@retry(max_attempts=3, backoff=2.0)
def fetch_url(url: str) -> str:
    response = requests.get(url, timeout=30)
    response.raise_for_status()
    return response.text

# Skip invalid data
for url in urls:
    try:
        content = fetch_url(url)
    except RequestException as e:
        logger.warning(f"Skipping {url}: {e}")
        continue
```

---

## ğŸ’¾ Storage & Caching

### Directory Structure

```
balkan-nlp/
â”œâ”€â”€ cache/          # HTTP response cache (gitignored)
â”œâ”€â”€ logs/           # Processing logs (gitignored)
â”œâ”€â”€ models/         # Local ML models (gitignored)
â”œâ”€â”€ output/         # Final datasets (gitignored)
â”‚   â”œâ”€â”€ clean_text/
â”‚   â”œâ”€â”€ language_id/
â”‚   â””â”€â”€ summarization/
â””â”€â”€ stats/          # Dataset statistics (gitignored)
```

### Caching Strategy

- Cache HTTP responses during development
- Invalidate cache after 7 days
- Store cache with URL hash as key
- Compress cached responses

---

## ğŸ” Security Boundaries

### Input Validation

- Validate URLs before fetching
- Sanitize filenames
- Validate YAML configs
- Check file sizes before processing

### Output Sanitization

- Remove PII (emails, phone numbers)
- Anonymize usernames
- Filter sensitive patterns
- Validate before HF upload

---

## ğŸ“Š Monitoring & Logging

### Logging Levels

- **DEBUG**: Detailed processing info
- **INFO**: Pipeline progress, statistics
- **WARNING**: Skipped items, validation issues
- **ERROR**: Failed operations, exceptions

### Metrics to Track

- Documents scraped per source
- Duplicate removal rate
- Processing time per step
- Output file sizes
- Error rates by type

### Example

```python
logger.info(f"Scraped {len(docs)} documents from {source}")
logger.info(f"Removed {dupes} duplicates ({dupes/len(docs)*100:.1f}%)")
logger.warning(f"Skipped {errors} URLs due to errors")
```

---

## ğŸ§ª Testing Strategy

### Unit Tests

- Individual functions in isolation
- Mock external dependencies
- Test edge cases

### Integration Tests

- End-to-end pipeline on sample data
- Validate output format
- Check metadata completeness

### Manual Testing

- Sample 1% of output for human review
- Verify source attribution
- Check for quality issues

---

## ğŸš€ Deployment

### Local Development

```bash
# Setup
python -m venv venv
source venv/bin/activate
pip install -e ".[dev]"

# Run pipeline
python scripts/run_clean_text.py --config datasets/clean_text/config.yaml
```

### Production (CI/CD)

```bash
# On server/GitHub Actions
pip install -e .
python scripts/run_clean_text.py --config config.yaml
```

---

## ğŸ“ˆ Scalability Considerations

### Current Design (Phase 1)

- Single-threaded processing
- Suitable for 50K-150K documents
- Processing time: hours to days

### Future Improvements (Phase 2+)

- Parallel scraping (multiprocessing)
- Distributed processing (Dask/Ray)
- Incremental updates
- Delta processing for new data

---

## ğŸ”— Related Documents

- [AGENTS.md](AGENTS.md) - AI agent instructions
- [METHODOLOGY.md](docs/METHODOLOGY.md) - Data processing methodology
- [SECURITY.md](SECURITY.md) - Security guidelines
- [CONTRIBUTING.md](CONTRIBUTING.md) - How to contribute

---

**Last Updated**: 2026-01-19
